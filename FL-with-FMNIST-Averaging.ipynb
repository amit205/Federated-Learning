{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "vw1 = sy.VirtualWorker(hook, id=\"vw1\")  # define remote worker vw1\n",
    "vw2 = sy.VirtualWorker(hook, id=\"vw2\")  # and vw2\n",
    "vw3 = sy.VirtualWorker(hook, id=\"vw3\")\n",
    "vw4 = sy.VirtualWorker(hook, id=\"vw4\")\n",
    "vw5 = sy.VirtualWorker(hook, id=\"vw5\")\n",
    "vw6 = sy.VirtualWorker(hook, id=\"vw6\")\n",
    "vw7 = sy.VirtualWorker(hook, id=\"vw7\")\n",
    "vw8 = sy.VirtualWorker(hook, id=\"vw8\")\n",
    "vw9 = sy.VirtualWorker(hook, id=\"vw9\")\n",
    "vw0 = sy.VirtualWorker(hook, id=\"vw0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.FashionMNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = Net().to(device)\n",
    "#Network = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.train()\n",
    "vw0_model = Network.copy().send(vw0)\n",
    "vw1_model = Network.copy().send(vw1)\n",
    "vw2_model = Network.copy().send(vw2)\n",
    "vw3_model = Network.copy().send(vw3)\n",
    "vw4_model = Network.copy().send(vw4)\n",
    "vw5_model = Network.copy().send(vw5)\n",
    "vw6_model = Network.copy().send(vw6)\n",
    "vw7_model = Network.copy().send(vw7)\n",
    "vw8_model = Network.copy().send(vw8)\n",
    "vw9_model = Network.copy().send(vw9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vw0_optimizer = optim.SGD(vw0_model.parameters(), lr=args.lr)\n",
    "vw1_optimizer = optim.SGD(vw1_model.parameters(), lr=args.lr)\n",
    "vw2_optimizer = optim.SGD(vw2_model.parameters(), lr=args.lr)\n",
    "vw3_optimizer = optim.SGD(vw3_model.parameters(), lr=args.lr)\n",
    "vw4_optimizer = optim.SGD(vw4_model.parameters(), lr=args.lr)\n",
    "vw5_optimizer = optim.SGD(vw5_model.parameters(), lr=args.lr)\n",
    "vw6_optimizer = optim.SGD(vw6_model.parameters(), lr=args.lr)\n",
    "vw7_optimizer = optim.SGD(vw7_model.parameters(), lr=args.lr)\n",
    "vw8_optimizer = optim.SGD(vw8_model.parameters(), lr=args.lr)\n",
    "vw9_optimizer = optim.SGD(vw9_model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_nodes = [vw0, vw1, vw2, vw3, vw4, vw5, vw6, vw7, vw8, vw9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list(),list(),list(),list(),list(),list(),list(),list(),list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [vw0_model, vw1_model, vw2_model, vw3_model, vw4_model, vw5_model, vw6_model, vw7_model, vw8_model, vw9_model]\n",
    "params = [list(vw0_model.parameters()), list(vw1_model.parameters()), list(vw2_model.parameters()),\n",
    "          list(vw3_model.parameters()), list(vw4_model.parameters()), list(vw5_model.parameters()),\n",
    "          list(vw6_model.parameters()), list(vw7_model.parameters()), list(vw8_model.parameters()),\n",
    "          list(vw9_model.parameters())]\n",
    "\n",
    "optimizers = [vw0_optimizer, vw1_optimizer, vw2_optimizer, vw3_optimizer, vw4_optimizer, vw5_optimizer, vw6_optimizer,\n",
    "              vw7_optimizer, vw8_optimizer, vw9_optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, Network, averaging_steps):    \n",
    "    Network.train()\n",
    "    for data_index in range(len(remote_dataset[0])-1):        \n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            models[remote_index] = Network.copy().send(compute_nodes[remote_index])\n",
    "            optimizers[remote_index] = optim.SGD(models[remote_index].parameters(), lr=args.lr)\n",
    "            for i in range(averaging_steps):\n",
    "                optimizers[remote_index].zero_grad()\n",
    "                pred = models[remote_index](data)\n",
    "                loss = F.nll_loss(pred, target)\n",
    "                loss.backward()\n",
    "                optimizers[remote_index].step()\n",
    "        with torch.no_grad():\n",
    "            for model in models:\n",
    "                model.move(secure_worker)\n",
    "            temp1, temp2, temp3, temp4, temp5, temp6, temp7, temp8 = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "            for i in range(len(compute_nodes)):\n",
    "                temp1 += models[i].conv1.weight.data\n",
    "                temp2 += models[i].conv1.bias.data\n",
    "                temp3 += models[i].conv2.weight.data\n",
    "                temp4 += models[i].conv2.bias.data\n",
    "                temp5 += models[i].fc1.weight.data\n",
    "                temp6 += models[i].fc1.bias.data\n",
    "                temp7 += models[i].fc2.weight.data\n",
    "                temp8 += models[i].fc2.bias.data\n",
    "            Network.conv1.weight.set_(temp1.get()/10)\n",
    "            Network.conv1.bias.set_(temp2.get()/10)\n",
    "            Network.conv2.weight.set_(temp3.get()/10)\n",
    "            Network.conv2.bias.set_(temp4.get()/10)\n",
    "            Network.fc1.weight.set_(temp5.get()/10)\n",
    "            Network.fc1.bias.set_(temp6.get()/10)\n",
    "            Network.fc2.weight.set_(temp7.get()/10)\n",
    "            Network.fc2.bias.set_(temp8.get()/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    for i in range(1):\n",
    "        Network.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = Network(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "\n",
      "Test set: Average loss: 0.7149, Accuracy: 7271/10000 (73%)\n",
      "\n",
      "Total 108.53 s\n",
      "Epoch 2\n",
      "\n",
      "Test set: Average loss: 0.6041, Accuracy: 7708/10000 (77%)\n",
      "\n",
      "Total 109.14 s\n",
      "Epoch 3\n",
      "\n",
      "Test set: Average loss: 0.5398, Accuracy: 8029/10000 (80%)\n",
      "\n",
      "Total 111.22 s\n",
      "Epoch 4\n",
      "\n",
      "Test set: Average loss: 0.4977, Accuracy: 8205/10000 (82%)\n",
      "\n",
      "Total 111.09 s\n",
      "Epoch 5\n",
      "\n",
      "Test set: Average loss: 0.4645, Accuracy: 8355/10000 (84%)\n",
      "\n",
      "Total 112.84 s\n",
      "Epoch 6\n",
      "\n",
      "Test set: Average loss: 0.4403, Accuracy: 8474/10000 (85%)\n",
      "\n",
      "Total 110.43 s\n",
      "Epoch 7\n",
      "\n",
      "Test set: Average loss: 0.4224, Accuracy: 8515/10000 (85%)\n",
      "\n",
      "Total 110.18 s\n",
      "Epoch 8\n",
      "\n",
      "Test set: Average loss: 0.4093, Accuracy: 8548/10000 (85%)\n",
      "\n",
      "Total 111.05 s\n",
      "Epoch 9\n",
      "\n",
      "Test set: Average loss: 0.3998, Accuracy: 8597/10000 (86%)\n",
      "\n",
      "Total 108.46 s\n",
      "Epoch 10\n",
      "\n",
      "Test set: Average loss: 0.3925, Accuracy: 8623/10000 (86%)\n",
      "\n",
      "Total 110.45 s\n"
     ]
    }
   ],
   "source": [
    "averaging_steps = 5\n",
    "for epoch in range(args.epochs):\n",
    "    t = time.time()\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train(epoch, Network, averaging_steps)\n",
    "    test()\n",
    "    total_time = time.time() - t\n",
    "    print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"fmnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
